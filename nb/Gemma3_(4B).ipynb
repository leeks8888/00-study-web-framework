{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeks8888/00-study-web-framework/blob/main/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# --- 1. Setup Environment ---\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Set Hugging Face Token (Optional, but recommended for Gemma models)\n",
        "# If the model is gated, you'll need to accept terms on its Hugging Face page\n",
        "# and provide a token with 'read' access.\n",
        "# from google.colab import userdata\n",
        "# hf_token = userdata.get('HF_TOKEN') # Store your token as a Colab secret\n",
        "# If not using Colab secrets, you can paste your token here, but it's less secure.\n",
        "# hf_token = \"YOUR_HUGGINGFACE_TOKEN\"\n",
        "# if 'hf_token' in locals():\n",
        "#     from huggingface_hub import login\n",
        "#     login(token=hf_token)\n",
        "\n",
        "# Check for GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"GPU not available. Training will be very slow. Consider enabling GPU in Runtime > Change runtime type.\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 2. Configuration ---\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Model Configuration\n",
        "# Replace with your desired Gemma model ID.\n",
        "# Original Gemma: \"google/gemma-7b-it\" or \"google/gemma-2b-it\"\n",
        "# Gemma 2: \"google/gemma-2-9b-it\" or \"google/gemma-2-2b-it\" (9B might be too large for free Colab tier)\n",
        "# If you have a specific \"Gemma 3\" ID, use it here.\n",
        "model_id = \"google/gemma-2-2b-it\" # Using 2B version for better Colab compatibility\n",
        "\n",
        "# PEFT Configuration (LoRA)\n",
        "lora_r = 16  # Rank of the update matrices\n",
        "lora_alpha = 32 # Alpha parameter for LoRA scaling\n",
        "lora_dropout = 0.05 # Dropout probability for LoRA layers\n",
        "# Target modules for Gemma (may vary slightly by specific Gemma version, check model card)\n",
        "# Common for Gemma:\n",
        "lora_target_modules = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "]\n",
        "\n",
        "# Training Arguments\n",
        "output_dir = \"./gemma_finetuned_level_generator\"\n",
        "num_train_epochs = 3  # Adjust as needed\n",
        "per_device_train_batch_size = 1 # Reduce if OOM errors, requires more gradient_accumulation_steps\n",
        "gradient_accumulation_steps = 4 # Increase if batch size is small\n",
        "learning_rate = 2e-4\n",
        "logging_steps = 25\n",
        "save_steps = 0 # Set to a positive integer if you want to save checkpoints during training\n",
        "save_strategy = \"epoch\" # Save at the end of each epoch\n",
        "max_seq_length = 1024 # Adjust based on your prompt/response lengths. Your JSONs can be long.\n",
        "# Check average and max length of your formatted prompts to set this appropriately.\n",
        "\n",
        "# Quantization Config\n",
        "use_4bit = True\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "bnb_4bit_compute_dtype = torch.bfloat16 # Use bfloat16 for Ampere GPUs and newer\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 3. Data Preparation ---\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Upload your sample.txt file to Colab\n",
        "# You can do this by clicking the \"Files\" icon on the left sidebar, then \"Upload\".\n",
        "# Make sure 'sample.txt' is in the root directory of your Colab environment.\n",
        "\n",
        "data_file_path = \"sample.txt\"\n",
        "\n",
        "if not os.path.exists(data_file_path):\n",
        "    print(f\"Error: {data_file_path} not found. Please upload it to your Colab environment.\")\n",
        "    # You might want to stop execution here if the file isn't present\n",
        "    # import sys\n",
        "    # sys.exit()\n",
        "else:\n",
        "    print(f\"Found {data_file_path}. Proceeding with data loading.\")\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_format_data(filepath):\n",
        "    prompts = []\n",
        "    responses = []\n",
        "    formatted_texts = []\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    # The 'prompt' field in your data is the actual input JSON string\n",
        "                    # The 'response' field is the actual output JSON string\n",
        "                    instruction = data['prompt']\n",
        "                    output = data['response']\n",
        "\n",
        "                    # Gemma instruction-following format (using the chat template)\n",
        "                    # You might need to adjust this based on the specific Gemma model's recommended prompt format\n",
        "                    # For -it (instruction-tuned) models, a direct instruction format is usually good.\n",
        "                    # A common format is <start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\n",
        "                    # For SFTTrainer, we typically format it as a single string.\n",
        "                    # Let's use a simplified instruction format:\n",
        "                    # text = f\"Instruction:\\nGenerate a game level configuration based on the following parameters.\\n\\nParameters:\\n{instruction}\\n\\nConfiguration:\\n{output}\"\n",
        "\n",
        "                    # Or using the specific chat template tokens if known for the model version\n",
        "                    # For gemma-2b-it and similar, the format is usually:\n",
        "                    # User: <prompt text>\n",
        "                    # Assistant: <response text>\n",
        "                    # SFTTrainer can often handle this with a dataset_text_field approach\n",
        "                    # A simple and effective format for SFTTrainer with Gemma often looks like:\n",
        "                    # \"<s>[INST] Instruction Text [/INST] Response Text </s>\"\n",
        "                    # Given your prompt is already structured, let's use it directly.\n",
        "                    text = f\"<s>[INST] {instruction} [/INST] {output} </s>\"\n",
        "                    formatted_texts.append({\"text\": text})\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Skipping line due to JSON decode error: {e} - Line: {line.strip()}\")\n",
        "                except KeyError as e:\n",
        "                    print(f\"Skipping line due to missing key: {e} - Line: {line.strip()}\")\n",
        "        return formatted_texts\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{filepath}' not found.\")\n",
        "        return []\n",
        "\n",
        "if os.path.exists(data_file_path):\n",
        "    formatted_data = load_and_format_data(data_file_path)\n",
        "    if formatted_data:\n",
        "        dataset = Dataset.from_list(formatted_data)\n",
        "        print(f\"\\nLoaded and formatted {len(dataset)} samples.\")\n",
        "        print(\"Sample formatted entry:\")\n",
        "        print(dataset[0]['text'][:500] + \"...\") # Print first 500 chars of the first sample\n",
        "\n",
        "        # Optional: Split into train/validation\n",
        "        # dataset = dataset.train_test_split(test_size=0.1)\n",
        "        # train_dataset = dataset[\"train\"]\n",
        "        # eval_dataset = dataset[\"test\"]\n",
        "        # For simplicity with small datasets, we can use the full dataset for training.\n",
        "        train_dataset = dataset\n",
        "        eval_dataset = None # Or set to a subset if you have a split\n",
        "    else:\n",
        "        print(\"No data loaded. Exiting.\")\n",
        "        # import sys\n",
        "        # sys.exit()\n",
        "else:\n",
        "    print(f\"{data_file_path} not found. Cannot proceed with training.\")\n",
        "    train_dataset = None # Ensure it's defined\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 4. Load Model and Tokenizer ---\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if train_dataset: # Only proceed if data is loaded\n",
        "    print(f\"\\nLoading model: {model_id}\")\n",
        "\n",
        "    compute_dtype = getattr(torch, \"bfloat16\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=use_4bit,\n",
        "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False, # Optional\n",
        "    )\n",
        "\n",
        "    # Load base model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config if use_4bit else None,\n",
        "        device_map=\"auto\", # Automatically distributes model across available GPUs\n",
        "        # token=hf_token # if using a token\n",
        "    )\n",
        "    model.config.use_cache = False # Recommended for fine-tuning\n",
        "    model.config.pretraining_tp = 1 # Optional: Set to 1 if not using tensor parallelism\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Gemma typically uses eos_token for padding\n",
        "    tokenizer.padding_side = \"right\" # Fixes weird overflow issue with fp16 training\n",
        "\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "else:\n",
        "    print(\"Skipping model loading as no training data is available.\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 5. PEFT Setup (LoRA) ---\n",
        "#-------------------------------------------------------------------------------\n",
        "if train_dataset and 'model' in locals(): # Check if model was loaded\n",
        "    print(\"\\nSetting up PEFT (LoRA)...\")\n",
        "    # Prepare model for k-bit training\n",
        "    if use_4bit: # Only if quantization is used\n",
        "      model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        target_modules=lora_target_modules,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "    print(\"PEFT model setup complete.\")\n",
        "else:\n",
        "    print(\"Skipping PEFT setup as model or data is not available.\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 6. Training ---\n",
        "#-------------------------------------------------------------------------------\n",
        "if train_dataset and 'model' in locals(): # Check if model and data are ready\n",
        "    print(\"\\nConfiguring training arguments...\")\n",
        "    training_arguments = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        optim=\"paged_adamw_8bit\" if use_4bit else \"adamw_torch\", # paged_adamw_8bit for 4-bit QLoRA\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=logging_steps,\n",
        "        save_strategy=save_strategy,\n",
        "        save_steps=save_steps if save_strategy == \"steps\" else 0,\n",
        "        fp16=True if device == \"cuda\" and not bnb_4bit_compute_dtype == \"torch.bfloat16\" else False, # Use fp16 if not using bfloat16\n",
        "        bf16=True if device == \"cuda\" and bnb_4bit_compute_dtype == \"torch.bfloat16\" else False,    # Use bf16 if Ampere and compute_dtype is bfloat16\n",
        "        max_grad_norm=0.3, # Gradient clipping\n",
        "        warmup_ratio=0.03, # Warmup ratio\n",
        "        lr_scheduler_type=\"constant\", # Or \"cosine\", \"linear\"\n",
        "        report_to=\"tensorboard\", # You can use \"wandb\" if you have it set up\n",
        "        # evaluation_strategy=\"steps\" if eval_dataset else \"no\",\n",
        "        # eval_steps=save_steps if eval_dataset else None, # Evaluate at the same frequency as saving\n",
        "    )\n",
        "\n",
        "    print(\"Initializing SFTTrainer...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset, # Pass eval_dataset here if you have one\n",
        "        peft_config=peft_config if 'peft_config' in locals() else None, # Pass LoRA config if PEFT is used\n",
        "        # dataset_text_field=\"text\", # Key in dataset that contains the formatted text\n",
        "        max_seq_length=max_seq_length,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        "        packing=False, # Set to True if your examples are short and you want to pack them\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # --- 7. Save Adapter ---\n",
        "    #-------------------------------------------------------------------------------\n",
        "    print(\"\\nSaving fine-tuned LoRA adapter...\")\n",
        "    adapter_model_path = os.path.join(output_dir, \"adapter_final\")\n",
        "    trainer.model.save_pretrained(adapter_model_path) # Saves only the LoRA adapter weights\n",
        "    tokenizer.save_pretrained(adapter_model_path) # Save tokenizer with adapter for easy loading\n",
        "    print(f\"Adapter saved to {adapter_model_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping training as model or data is not available.\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 8. Inference (Example) ---\n",
        "#-------------------------------------------------------------------------------\n",
        "if os.path.exists(os.path.join(output_dir, \"adapter_final\")) and 'model_id' in locals(): # Check if adapter and model_id exist\n",
        "    print(\"\\n--- Testing Fine-tuned Model ---\")\n",
        "\n",
        "    # Load the base model again (quantized or full-precision)\n",
        "    # Important: If you used quantization during training, load with same quantization for inference.\n",
        "    base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config if use_4bit else None,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=getattr(torch, \"bfloat16\") if use_4bit else torch.float16, # Match training\n",
        "        # token=hf_token # if using a token\n",
        "    )\n",
        "\n",
        "    # Load the LoRA adapter\n",
        "    # Make sure the adapter_model_path is correct (where you saved the adapter)\n",
        "    final_adapter_path = os.path.join(output_dir, \"adapter_final\")\n",
        "    ft_model = PeftModel.from_pretrained(base_model_for_inference, final_adapter_path)\n",
        "    ft_model = ft_model.eval() # Set to evaluation mode\n",
        "\n",
        "    # Load tokenizer associated with the adapter\n",
        "    ft_tokenizer = AutoTokenizer.from_pretrained(final_adapter_path)\n",
        "\n",
        "    # Example prompt from your data structure (or a new one)\n",
        "    # Use one of the prompts from your sample.txt for a realistic test\n",
        "    # This should be JUST the JSON string that was in the \"prompt\" field of your sample.txt\n",
        "    example_idx_to_test = 0 # Test with the first example's prompt\n",
        "    if os.path.exists(data_file_path):\n",
        "        with open(data_file_path, 'r', encoding='utf-8') as f:\n",
        "            test_lines = f.readlines()\n",
        "        if test_lines and len(test_lines) > example_idx_to_test:\n",
        "            try:\n",
        "                test_prompt_data = json.loads(test_lines[example_idx_to_test])['prompt']\n",
        "                actual_response_data = json.loads(test_lines[example_idx_to_test])['response']\n",
        "            except (json.JSONDecodeError, KeyError):\n",
        "                print(\"Could not parse test prompt from file, using a placeholder.\")\n",
        "                test_prompt_data = '{\"level_idx\": 1, \"fail_cnt\": 0.036, \"is_hard\": false, \" star1\": \"700\", \" star2\": \"6630\", \" star3\": \"9900\", \" move\": \"30\", \"color\": 3, \"R\": 1, \"G\": 1, \"B\": 1, \"O\": 0, \"PI\": 0, \"S\": 0, \"Y\": 0, \"PU\": 0, \"bubble\": 70, \"row\": 7, \"t_기본버블\": 70, \"t_스파크\": 0, \"t_강철\": 0, \"t_얼음\": 0, \"t_블랙홀\": 0, \"t_변신\": 0, \"t_유령\": 0, \"t_구름\": 0, \"t_나무\": 0, \"t_스위치블랙홀\": 0, \"t_마이너스플러스\": 0, \"t_듀오\": 0, \"t_물감\": 0, \"t_fixed\": 0}'\n",
        "                actual_response_data = \"Unknown\"\n",
        "\n",
        "    else: # Fallback if sample.txt is not found during inference part\n",
        "        test_prompt_data = '{\"level_idx\": 1, \"fail_cnt\": 0.036, \"is_hard\": false, \" star1\": \"700\", \" star2\": \"6630\", \" star3\": \"9900\", \" move\": \"30\", \"color\": 3, \"R\": 1, \"G\": 1, \"B\": 1, \"O\": 0, \"PI\": 0, \"S\": 0, \"Y\": 0, \"PU\": 0, \"bubble\": 70, \"row\": 7, \"t_기본버블\": 70, \"t_스파크\": 0, \"t_강철\": 0, \"t_얼음\": 0, \"t_블랙홀\": 0, \"t_변신\": 0, \"t_유령\": 0, \"t_구름\": 0, \"t_나무\": 0, \"t_스위치블랙홀\": 0, \"t_마이너스플러스\": 0, \"t_듀오\": 0, \"t_물감\": 0, \"t_fixed\": 0}'\n",
        "        actual_response_data = \"Unknown\"\n",
        "\n",
        "    # Format for inference - it must match the training format UP TO the response part.\n",
        "    # Training format: \"<s>[INST] {instruction} [/INST] {output} </s>\"\n",
        "    # Inference format: \"<s>[INST] {instruction} [/INST]\"\n",
        "    inference_prompt = f\"<s>[INST] {test_prompt_data} [/INST]\"\n",
        "\n",
        "    print(f\"\\nTest Prompt:\\n{test_prompt_data}\")\n",
        "    if actual_response_data != \"Unknown\":\n",
        "        print(f\"\\nActual Response (from data):\\n{actual_response_data[:300]}...\") # Show beginning of actual response\n",
        "\n",
        "    input_ids = ft_tokenizer(inference_prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).input_ids.to(device)\n",
        "\n",
        "    # Generate response\n",
        "    # Adjust max_new_tokens based on the expected length of your JSON responses\n",
        "    # Your responses are quite long, so this might need to be 512 or more.\n",
        "    print(\"\\nGenerating response...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = ft_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=768, # Increased for potentially long JSON output\n",
        "            do_sample=True,     # Sample for more diverse outputs\n",
        "            temperature=0.6,    # Lower for more deterministic, higher for more creative\n",
        "            top_p=0.9,          # Nucleus sampling\n",
        "            eos_token_id=ft_tokenizer.eos_token_id,\n",
        "            pad_token_id=ft_tokenizer.pad_token_id # Ensure pad_token_id is set\n",
        "        )\n",
        "\n",
        "    generated_text = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # The output will contain the prompt. We need to extract just the model's response part.\n",
        "    # Response starts after \"[/INST]\"\n",
        "    response_part = generated_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    print(f\"\\nGenerated Response (JSON String):\\n{response_part}\")\n",
        "\n",
        "    # Optional: Try to parse the generated JSON to see if it's valid\n",
        "    try:\n",
        "        parsed_generated_response = json.loads(response_part)\n",
        "        print(\"\\nSuccessfully parsed generated JSON response:\")\n",
        "        # print(json.dumps(parsed_generated_response, indent=2)) # Pretty print\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"\\nCould not parse generated response as JSON: {e}\")\n",
        "\n",
        "    # --- If you want to use a pipeline for simpler inference ---\n",
        "    # print(\"\\n--- Testing with pipeline ---\")\n",
        "    # pipe = pipeline(task=\"text-generation\", model=ft_model, tokenizer=ft_tokenizer, device_map=\"auto\")\n",
        "    # result = pipe(inference_prompt, max_new_tokens=768, do_sample=True, temperature=0.6, top_p=0.9)\n",
        "    # generated_text_pipe = result[0]['generated_text']\n",
        "    # response_part_pipe = generated_text_pipe.split(\"[/INST]\")[-1].strip()\n",
        "    # print(f\"\\nGenerated Response (Pipeline):\\n{response_part_pipe}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping inference as fine-tuned model adapter or model_id is not available.\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# --- 9. (Optional) Merge and Save Full Model ---\n",
        "#-------------------------------------------------------------------------------\n",
        "# If you want to merge the LoRA adapter with the base model to create a standalone\n",
        "# fine-tuned model, you can do the following. This will require more disk space\n",
        "# and memory to load.\n",
        "\n",
        "# print(\"\\n--- (Optional) Merging LoRA adapter with base model ---\")\n",
        "# if os.path.exists(os.path.join(output_dir, \"adapter_final\")) and 'model_id' in locals():\n",
        "#     # Load base model without quantization for merging, or ensure it's compatible\n",
        "#     # Merging works best if the base model is loaded in full or compatible precision (e.g., float16)\n",
        "#     base_model_for_merging = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_id,\n",
        "#         torch_dtype=torch.float16, # or torch.bfloat16\n",
        "#         device_map=\"auto\", # Use CPU or a single GPU to avoid device map issues during merge\n",
        "#         # token=hf_token # if using a token\n",
        "#     )\n",
        "#\n",
        "#     # Load PEFT model (adapter)\n",
        "#     merged_model = PeftModel.from_pretrained(base_model_for_merging, os.path.join(output_dir, \"adapter_final\"))\n",
        "#\n",
        "#     # Merge the adapter into the base model\n",
        "#     print(\"Merging model...\")\n",
        "#     merged_model = merged_model.merge_and_unload()\n",
        "#     print(\"Merge complete.\")\n",
        "#\n",
        "#     # Save the merged model\n",
        "#     merged_model_path = os.path.join(output_dir, \"merged_model_final\")\n",
        "#     merged_model.save_pretrained(merged_model_path)\n",
        "#     tokenizer.save_pretrained(merged_model_path) # Also save tokenizer with it\n",
        "#     print(f\"Merged model saved to {merged_model_path}\")\n",
        "#\n",
        "#     # You can then load this merged_model directly:\n",
        "#     # from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#     # loaded_merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path)\n",
        "#     # loaded_tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
        "# else:\n",
        "#     print(\"Skipping model merging as adapter or base model_id is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625,
          "referenced_widgets": [
            "ee1d1eb2892044129373b11a54c43670",
            "b53bc5b211644f38abec06e5163181fa",
            "06a273e9eb7a4953bb85678afc63d62a",
            "024a2097445148fc9c59b5d41685c0df",
            "211cff9df0c140a3b1939d5408300444",
            "425047606efe421b8000cd21069a0ddb",
            "90cfc65e9074493082b19180118a090a",
            "c7025de0e097452fa189e5b2113f2583",
            "1268006fc3d14db2a2baeeb019b7a69d",
            "76d7d50304a74fb99e50cc9e63775217",
            "2da89c74805541d0b5cfb4a4beaac728"
          ]
        },
        "id": "8GzPD-XftqbH",
        "outputId": "a4434e16-482b-4046-ae6a-e176ca2cd9f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 is available.\n",
            "Found sample.txt. Proceeding with data loading.\n",
            "\n",
            "Loaded and formatted 10 samples.\n",
            "Sample formatted entry:\n",
            "<s>[INST] {\"level_idx\": 1, \"fail_cnt\": 0.036, \"is_hard\": false, \" star1\": \"700\", \" star2\": \"6630\", \" star3\": \"9900\", \" move\": \"30\", \"color\": 3, \"R\": 1, \"G\": 1, \"B\": 1, \"O\": 0, \"PI\": 0, \"S\": 0, \"Y\": 0, \"PU\": 0, \"bubble\": 70, \"row\": 7, \"t_기본버블\": 70, \"t_스파크\": 0, \"t_강철\": 0, \"t_얼음\": 0, \"t_블랙홀\": 0, \"t_변신\": 0, \"t_유령\": 0, \"t_구름\": 0, \"t_나무\": 0, \"t_스위치블랙홀\": 0, \"t_마이너스플러스\": 0, \"t_듀오\": 0, \"t_물감\": 0, \"t_fixed\": 0}\n",
            " [/INST] {\"author\":\"whlim\",\"index\":1,\"bubbles\":[{\"x\":0,\"y\":0,\"c\":1,\"t\":0},{\"x\":1,\"y\":0,\"c\":1,\"t...\n",
            "\n",
            "Loading model: google/gemma-2-2b-it\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee1d1eb2892044129373b11a54c43670"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded.\n",
            "\n",
            "Setting up PEFT (LoRA)...\n",
            "trainable params: 20,766,720 || all params: 2,635,108,608 || trainable%: 0.7881\n",
            "PEFT model setup complete.\n",
            "\n",
            "Configuring training arguments...\n",
            "Initializing SFTTrainer...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'max_seq_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7a4cde091274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing SFTTrainer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'max_seq_length'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee1d1eb2892044129373b11a54c43670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b53bc5b211644f38abec06e5163181fa",
              "IPY_MODEL_06a273e9eb7a4953bb85678afc63d62a",
              "IPY_MODEL_024a2097445148fc9c59b5d41685c0df"
            ],
            "layout": "IPY_MODEL_211cff9df0c140a3b1939d5408300444"
          }
        },
        "b53bc5b211644f38abec06e5163181fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_425047606efe421b8000cd21069a0ddb",
            "placeholder": "​",
            "style": "IPY_MODEL_90cfc65e9074493082b19180118a090a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "06a273e9eb7a4953bb85678afc63d62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7025de0e097452fa189e5b2113f2583",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1268006fc3d14db2a2baeeb019b7a69d",
            "value": 2
          }
        },
        "024a2097445148fc9c59b5d41685c0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d7d50304a74fb99e50cc9e63775217",
            "placeholder": "​",
            "style": "IPY_MODEL_2da89c74805541d0b5cfb4a4beaac728",
            "value": " 2/2 [00:24&lt;00:00, 10.24s/it]"
          }
        },
        "211cff9df0c140a3b1939d5408300444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425047606efe421b8000cd21069a0ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cfc65e9074493082b19180118a090a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7025de0e097452fa189e5b2113f2583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1268006fc3d14db2a2baeeb019b7a69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76d7d50304a74fb99e50cc9e63775217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da89c74805541d0b5cfb4a4beaac728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}